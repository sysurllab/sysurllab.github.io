<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>yuchao.tech</title><meta name="author" content="余超(Yu Chao)"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">yuchao.tech</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/Home/home.html"> Home</a></li><li class="menus_item"><a class="site-page" href="/Publications/publications.html"> Publications</a></li><li class="menus_item"><a class="site-page" href="/"> About</a></li><li class="menus_item"><a class="site-page" href="/"> Contact Us</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/yuchao.png" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>余超(Yu Chao)</h3><p class="author-bio">中山大学计算机学院先进网络与计算系统研究所副教授</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://github.com/sysurllab" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="mailto:yuchao3@mail.sysu.edu.cn" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><article><h2 id="个人介绍"><a href="#个人介绍" class="headerlink" title="个人介绍"></a>个人介绍</h2><p>余超，博士生导师，中山大学“百人计划”引进副教授，国家“香江学者”。2007年本科毕业于华中科技大学电信系，获通信工程学士学位，2013年博士毕业于澳大利亚伍伦贡大学计算机系，获计算机博士学位。主要从事强化学习理论与应用研究工作。先后在IEEE TNNLS（IF:11.683）, IEEE TCB（IF:10.387），IEEE ITS（IF:4.051）, IEEE TVT（IF:4.432），ACM T等国际期刊和ICML&#x2F;NeurlPS&#x2F;IJCAI&#x2F;AAMAS&#x2F;AAAI上发表学术论文100余篇。主持科研项目20余项。获最佳论文奖3次。</p>
<p>担任SCI期刊IEICE TIS和JSSE副主编；组织国际会议MATCSD2015，ACAN 2016，IEEE ICA2016，IEEE ICA2017，Special Track in PRICA20I8，DAI2019, DAI2020；担任国际会议IJCAI2020, AAAI2020, IJCAI2019，AAMAS2019，AAAI2019，AAMAS2018，PRICAI2018，AAMAS2017等国际会议PC。</p>
<h2 id="实验室介绍"><a href="#实验室介绍" class="headerlink" title="实验室介绍"></a>实验室介绍</h2><p>强化学习是当前人工智能热潮的核心技术之一，在机器人与多机器人系统、健康医疗、广告推荐、对话系统、经济博弈等领域有着广泛的应用。目前，我们团队主要的科研项目有：（1）基于强化学习的机器人行为控制（深度强化学习、人机交互、数字孪生）；（2）基于分布式强化学习的智能集群协同（智能集群、多机器人系统、智能网联汽车）；（3）基于强化学习的慢性病和重症室（脓毒症等）最优治疗；（4）非完全信息博弈（德扑麻将棋牌类游戏、王者荣耀Dota等战略游戏、博弈对抗等）；（5）强化学习理论（模型学习、探索和信任分配、多智能体强化学习、离线学习、分层学习、贝叶斯学习等）。</p>
<p>本实验室目前承担多项国家级课题，并与腾讯、华为、中山大学附属医院等企业机构建立校企合作平台，表现优异的学生有机会派遣至优秀互联网企业进行实习工作，或推荐至香港、澳大利亚、美国、日本、新加坡等众多高校攻读博士，欢迎有志于从事人工智能前沿研究的优秀本科生和研究生加入实验室。</p>
<h2 id="研究领域"><a href="#研究领域" class="headerlink" title="研究领域"></a>研究领域</h2><p>（1）智能体与多智能体系统理论：强化学习、多智能体强化学习、博弈论、计算经济学、社会选择理论</p>
<p>（2）智能体与多智能体系统应用：自动驾驶与智能网联汽车、机器人与多机器人系统、智能电网、量化交易</p>
<p>（3）基于强化学习的机器人行为控制：深度强化学习、人机交互、迁移学习、虚实混合（数字孪生）、模型学习</p>
<p>（4）基于强化学习的博弈对抗技术：智能集群、非完全信息博弈（实时战略游戏、棋牌）</p>
<p>（5）基于强化学习的智能医疗：慢性病治疗、重症室（脓毒症、呼吸机）决策、健康管理、运动康复</p>
<h2 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h2><ul>
<li>2014.3-2016.12， 大连理工大学&#x2F;计算机学院，讲师</li>
<li>2016.12-2019.12，大连理工大学&#x2F;计算机学院，副教授（破格）</li>
<li>2019.12-今，中山大学&#x2F;数据科学与计算机学院，副教授</li>
</ul>
<h2 id="海外经历"><a href="#海外经历" class="headerlink" title="海外经历"></a>海外经历</h2><ul>
<li>2018.1-2019.6， 香港浸会大学&#x2F;计算机系，研究员</li>
<li>2010.9-2013.12，澳大利亚伍伦贡大学&#x2F;计算机与软件工程系，博士</li>
</ul>
<h2 id="获奖及荣誉"><a href="#获奖及荣誉" class="headerlink" title="获奖及荣誉"></a>获奖及荣誉</h2><ul>
<li>国家“香江学者”</li>
<li>IEEE Conference on Games机器人迁移强化学习挑战赛冠军</li>
<li>大连市高层次创新人才</li>
<li>大连理工“星海学者”</li>
<li>辽宁省自然科学学术成果奖（论文类）三等奖, 2018</li>
<li>辽宁省自然科学学术成果奖（论文类）二等奖, 2016, 2017</li>
<li>大连市自然科学优秀学术论文奖二等奖, 2017</li>
<li>大连市自然科学优秀学术论文奖一等奖, 2016</li>
<li>大连理工大学教学质量优良奖, 2016</li>
<li>大连理工大学“优秀党员“, 2016</li>
<li>大连理工大学“优秀工会工作积极分子”, 2016，2017</li>
</ul>
<h2 id="科研项目"><a href="#科研项目" class="headerlink" title="科研项目"></a>科研项目</h2><p>主持项目：20余项</p>
<h2 id="主要学术兼职"><a href="#主要学术兼职" class="headerlink" title="主要学术兼职"></a>主要学术兼职</h2><p><strong>Editor</strong></p>
<p>​    • Special Section on Frontiers in Agent-based Technology, IEICE Trans. Information and Systems.</p>
<p>​    • Special Issue on Agent-Based Modelling for Complex Systems, J. Systems Science and Engineering.</p>
<p><strong>Organisers</strong></p>
<p>​    • Workshop on Reinforcement Learning at DAI’19</p>
<p>   • Workshop on Methods and Applications of Reinforcement Learning @ PRICAI2018, Nanjing, China, August, 2018.</p>
<p>​    • Special Track on Reinforcement Learning @ PRICAI2018, Nanjing, China, August, 2018.</p>
<p>   • IEEE ICA2017, 2nd IEEE International Conference on Agents (2017 IEEE ICA), Beijing, China, July 6-9, 2017.</p>
<p>   • IEEE ICA2016, IEEE International Conference on Agents (2016 IEEE ICA), Metsue, Japan, September 28-30, 2016.</p>
<p>   • ACAN 2016, The 8th International Workshop on Agent-based Complex Automated Negotiations (ACAN2016@AAMAS2016), Singapore, May 4, 2016.</p>
<p>   • MATCSD2015, Multi-Agent Technologies for Complex Systems Development: Challenges and Solutions, Dalian University of Technology, China, September 17-18, 2015.</p>
<p><strong>PC</strong></p>
<p>​    IJCAI2019，AAMAS2019，AAAI2019，AAMAS2018，PRICAI2018，AAMAS2017</p>
<h2 id="教授课程"><a href="#教授课程" class="headerlink" title="教授课程"></a>教授课程</h2><ol>
<li>《强化学习原理及应用》</li>
<li>《人工智能》、《人工智能实验》、《人工智能实践》</li>
<li>《多智能体系统》</li>
<li>《推理与学习》</li>
<li>《汇编语言》</li>
<li>《图论以及应用》</li>
</ol>
<h2 id="代表性论著"><a href="#代表性论著" class="headerlink" title="代表性论著:"></a>代表性论著:</h2><p>期刊论文</p>
<ol>
<li>余超, 董银昭, 郭宪, 冯旸赫, 卓汉逵, 张强，一种基于结构交互驱动的机器人深度强化学习控制方法，软件学报，2023</li>
<li>Chao Yu, Qikai Huang, Towards more efficient and robust evaluation of sepsis treatment with deep reinforcement learning, BMC Medical Informatics and Decision Making, 2023</li>
<li>Chao Yu, JIming Liu and Shamim Nemati. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.08796v1">Reinforcement Learning in Healthcare: A Survey</a> ACM Computing Survey, 2021.</li>
<li>Chao Yu, et al. Supervised-actor-critic reinforcement learning for intelligent mechanical ventilation and sedative dosing in intensive care units. BMC Medical Informatics and Decision Making, 2020 (IF:2.134)</li>
<li>Chao Yu, Yinzhao Dong, Yangning Li, Yatong Chen Distributed multi-agent deep reinforcement learning for cooperative multi-robot pursuit , The Journal of Engineering, 2020.</li>
<li>Chao Yu, Xin Wang, Xin Xu, et al. Distributed Multiagent Coordinated Learning for Autonomous Driving in Highways Based on Dynamic Coordination Graphs. <strong>IEEE Transactions Intelligent Transportation Systems</strong>, doi: 10.1109&#x2F;TITS.2019.2893683, 2019. (IF:4.051)</li>
<li>Chao Yu, Jiming Liu and Hongyi Zhao. Inverse Reinforcement Learning for Intelligent Mechanical Ventilation and Sedative Dosing in Intensive Care Units. BMC Medical Informatics and Decision Making, 2019. (IF:2.134)</li>
<li>Chao Yu, Yinzhao Dong and Jiming Liu, and Guoqi Ren. Incorporating Causal Factors into Reinforcement Learning for Dynamic Treatment Regimes in HIV. BMC Medical Informatics and Decision Making, 2019. (IF:2.134)</li>
<li>Bingcai Chen, Chao Yu*, Qishaui Diao, Rui Liu and Yuliang Wang. Social or Individual Learning? An Aggregated Solution for Coordination in Multiagent Systems. Journal of Systems Science and Systems Engineering, 27 (2), 180-200 (IF:0.766)</li>
<li>Fuxin Zhang, Guozhen Tan, Chao Yu. Fair Transmission Rate Adjustment in Cooperative Vehicle Safety Systems based on Multi-Agent Model Predictive Control. <strong>IEEE Transactions on Vehicular Technology.</strong> 66(7): 6115-6129, 2017. (IF:4.432)</li>
<li>Chao Yu, Minjie Zhang, Fenghui Ren, and Guozhen Tan. Emotional Multiagent Reinforcement Learning in Spatial Social Dilemmas, <strong>IEEE Transactions on Neural Networks and Learning Systems</strong>. 26(12), 3083-3096, 2015. (<strong>4.051</strong>)</li>
<li>Chao Yu, Minjie Zhang, Fenghui Ren, and Guozhen Tan. Multiagent Learning of Coordination in Loosely Coupled Multiagent Systems, <strong>IEEE Transactions on Cybernetics</strong>. 45(12), 2853-2867, 2015. (<strong>IF:10.387</strong>）</li>
<li>Chao Yu, Minjie Zhang and Fenghui Ren and Guozhen Tan. Emergence of Social Norms through Collective Learning in Networked Multiagent Systems, <strong>IEEE Transactions on Cybernetics</strong>, 44(12): 2342-2355, 2014. (<strong>IF:10.387</strong>)</li>
</ol>
<p>会议论文</p>
<ol>
<li>Qian Lin, Bo Tang, Zifan Wu, Chao Yu*, et al. Safe Offline Reinforcement Learning with Real-Time Budget Constraints, ICML2023</li>
<li>Wenxuan Zhu, Chao Yu*, Qiang Zhang. Causal Deep Reinforcement Learning using Observational Data, IJCAI2023</li>
<li>Chao Yu, Hierarchical Mean-Field Deep Reinforcement Learning for Large-Scale Multiagent Systems, AAAI2023</li>
<li>Zifan Wu, Chao Yu*, et al. Models as Agents: Optimizing Multi-Step Predictions of Interactive Local Models in Model-Based Multi-Agent Reinforcement Learning, AAAI2023</li>
<li>Pei Xu, Junge Zhang, Qiyue Yin, Chao Yu, Yaodong Yang, Kaiqi Huang. Subspace-Aware Exploration for Sparse-Reward Multi-Agent Tasks, AAAI2023</li>
<li>Yucong Zhang, Chao Yu*, et al. EXPODE: EXploiting POlicy Discrepancy for Efficient Exploration in Multi-agent Reinforcement Learning, AAMAS2023</li>
<li>Zongkai Liu, Chao Yu*, et al. A Unified Diversity Measure for Multiagent Reinforcement Learning, NeurlPS2022.</li>
<li>Zifan Wu, Chao Yu*, et al. Plan To Predict: Learning an Uncertainty-Foreseeing Model For Model-Based Reinforcement Learning, NeurlPS2022.</li>
<li>Mu Jin, Zhihao Ma, Kebin Jin, Hankui Zhuo, Chen Chen, Chao Yu, SORL: Automatic Symbolic Option Discovery for Facilitating Deep Reinforcement Learning, AAAI2022</li>
<li>Zifan Wu, Chao Yu*, et al. Coordinated Proximal Policy Optimization, NeurlPS2021.</li>
<li>Chao Yu, et al. Decomposed Deep Reinforcement Learning for Robotic Control, AAMAS2020.</li>
<li>Chao Yu, et al. Interactive RL via Online Human Demonstrations, AAMAS2020.</li>
<li>Chao Yu, Guozhen Tan, The Price of Governance: A Middle Ground Solution to Coordination in Organizational Control, IJCAI2019.</li>
<li>Yaodong yang, Jianye Hao and Chao Yu, Large-Scale Home Energy Management Using Entropy-Based Collective Multiagent Reinforcement Learning Framework. IJCAI2019</li>
<li>Chao Yu, Xin Wang, Zhanbo Feng: Coordinated Multiagent Reinforcement Learning for Teams of Mobile Sensing Robots. AAMAS 2019: 2297-2299</li>
<li>Chao Yu, Guoqi Ren and Jiming Liu, Deep Inverse Reinforcement Learning for Sepsis Treatment, 2019 IEEE International Conference on Healthcare Informatics, 2019. (EI)</li>
<li>Chao Yu, Yinzhao Dong and Xin Wang, Multiagent Reinforcement Learning on Coordination Graphs, 4th International Workshop on Smart Simulation and Modelling for Complex Systems (<a href="mailto:SSMCS@IJCAI">SSMCS@IJCAI</a> 2019). (<strong>Best Paper Award****）</strong></li>
<li>Chao Yu, Dongxu Wang, Jiankang Ren, Hongwei Ge and Liang Sun. Decentralized Multiagent Reinforcement Learning for Efficient Robotic Control by Coordination Graphs. 15th Pacific Rim International Conference on Artificial Intelligence, pp. 191-203, 2018.</li>
<li>Chao Yu, Dongxu Wang, Tianpei Yang, Wenxuan Zhu, Yuchen Li, Hongwei Ge and Jiankang Ren. Adaptively Shaping Reinforcement Learning Agents via Human Reward. 15th Pacific Rim International Conference on Artificial Intelligence, pp. 85-97, 2018. (<strong>Best Paper Nomination</strong>, 5 out of 441)</li>
<li>Chao Yu, Yatong Chen, Hongtao Lv, Jiankang Ren, Hongwei Ge and Liang Sun. Neural learning for the emergence of social norms in multiagent systems. 2017 IEEE International Conference on Agents (ICA), pp. 40-45, 2017.</li>
<li>Chao Yu, Hongtao Lv, Sandip Sen, Jianye hao, Fenghui Ren and Rui Liu. An Adaptive Learning Framework for Efficient Emergence of Social Norms. 15th International Conference on Autonomous Agents and Multiagent Systems (AAMAS2016), Singapore. pp. 1307-1308, 2016.</li>
<li>Chao Yu, Hongtao Lv, Sandip Sen, Fenghui Ren and Guozhen Tan. Adaptive Learning for Efficient Emergence of Social Norms in Networked Multiagent Systems. In The Proceedings of the 14th Pacific Rim International Conference on Artificial Intelligence (PRICAI 2016): Trends in Artificial Intelligence. LNAI 9810, pp. 805-818, 2016.</li>
<li>Chao Yu, Minjie Zhang, Fenghui Ren and Xudong Luo. Emergence of Social Norms Through Collective Learning in Networked Agent Societies. The Twelfth International Conference on Autonomous Agents and Multiagent Systems (AAMAS2013) , pp.475-482, May 6-10, 2013, Saint Paul, USA.</li>
<li>Chao Yu, Fenghui Ren and Minjie Zhang. An Adaptive Bilateral Negotiation Model Based on Bayesian Learning. The 4th AAMAS International Workshop on Agent-based Complex Automated Negotiations (ACAN@AAMAS2011), <strong>The Best Student Paper Award</strong>, Taipei, 2011</li>
</ol>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/Home/home.html"> Home</a></li><li class="nav_item"><a class="nav-page" href="/Publications/publications.html"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/"> About</a></li><li class="nav_item"><a class="nav-page" href="/"> Contact Us</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2023 by 余超(Yu Chao)</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>